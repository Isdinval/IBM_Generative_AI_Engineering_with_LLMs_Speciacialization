# Generative AI Language Modeling with Transformers 🚀  

## 📌 Aperçu du cours  
Ce cours vous permet d'utiliser des modèles basés sur les *transformers* pour le **traitement du langage naturel (NLP)**. Vous apprendrez à :  
- Expliquer les mécanismes d'*attention* et leur rôle dans la modélisation contextuelle.  
- Implémenter des modèles comme **GPT (décodeur)** et **BERT (encodeur)** avec PyTorch.  
- Appliquer les transformers pour la classification de texte, la traduction et la modélisation linguistique.  

---

## 🧠 Modules & Contenu  

### **Module 1 : Concepts Fondamentaux des Transformers**  
#### Contenu des notebooks :  
1. **Positional Encoding** :  
   - Techniques pour encoder la position des mots dans une séquence.  
   - Implémentation avec PyTorch.  

2. **Mécanismes d'Attention** :  
   - Fonctionnement de l'*attention* et application aux embeddings de mots.  
   - *Self-attention* pour prédire des tokens.  

3. **Transformers pour la Classification** :  
   - Architecture des encodeurs.  
   - Création d'un pipeline de texte et entraînement du modèle.  

🔹 **Activités pratiques** :  
- *Lab* : Mécanismes d'attention et *positional encoding*.  
- *Lab* : Application des transformers pour la classification.  

---

### **Module 2 : Modèles Avancés et Applications**  
#### Contenu des notebooks :  
4. **Modèles Décodeurs (GPT-like)** :  
   - Principe des modèles génératifs (ex : GPT).  
   - Implémentation en PyTorch (Causal LM).  

5. **Modèles Encodeurs (BERT)** :  
   - Prétraining avec *Masked Language Modeling (MLM)* et *Next Sentence Prediction (NSP)*.  
   - Préparation des données pour BERT.  

6. **Transformers pour la Traduction** :  
   - Architecture complète (encodeur-décodeur).  
   - Implémentation PyTorch.  

🔹 **Activités pratiques** :  
- *Lab* : Modèles GPT-like.  
- *Lab* : Prétraining de BERT.  
- *Lab* : Traduction avec transformers.  

---

## 🛠️ Compétences Acquises  
- **Generative AI** 🤖  
- **PyTorch** 🏗️  
- **Deep Learning & NLP** 📚  
- **Modélisation Langagière (LLMs)** ✍️  
